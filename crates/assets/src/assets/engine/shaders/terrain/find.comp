#version 460 core
layout (local_size_x = 32, local_size_y = 1, local_size_z = 1) in;

#include "sub_allocations"
#include "triangles_per_sub_allocation"
#include "vertices_per_sub_allocation"

// Contains the chunk index
layout(push_constant) uniform PushConstants {
    uint chunk_index;
} push_constants;

// Sub allocation chunk indices
layout(std430, set = 0, binding = 0) buffer SubAllocationChunkIndices {
    uint[sub_allocations] data;
} indices;

// Allocation offsets
layout(std430, set = 0, binding = 1) buffer FoundOffsets {
    uint vertices;
    uint triangles;
} offsets;

// Atomic counters
layout(std430, set = 0, binding = 2) readonly buffer Counters {
    uint vertices;
    uint triangles;
} counters;

// Must be shared for atomic ops between groups
shared uint chosen_sub_allocation_index;

// Sub-allocations:         [-1] [-1] [3] [3] [3] [-1] [2] [2]
// Sub-allocation groups:   [               ] [              ]
void main() {
    // Dispatch invocations are sub allocation GROUPS btw
    if (gl_GlobalInvocationID.x > (sub_allocations / 4)) {
        return;
    }

    // Checks if we are within a free region or not
    bool within_free = indices.data[0] == uint(-1);

    // Keeps count of the number of empty sub allocations that we passed through 
    uint free_sub_allocations = 0;

    // Temp values for now 
    uint temp_sub_allocation_index = 0;
    uint temp_sub_allocation_count = 1;

    uint invocation_local_chosen_sub_alloction_index = 0;

    // If we are the first group, update temporarily
    if (gl_GlobalInvocationID.x == 0) {
        atomicExchange(chosen_sub_allocation_index, uint(-1));
    }

    // Find a free memory range for this specific sub-allocation GROUP
    uint base = gl_GlobalInvocationID.x * 32;
    for (uint i = base; i < (base + 32); i++) {
        bool free = indices.data[i] == uint(-1);
        
        // We just moved into a free allocation
        if (!within_free && free) {
            temp_sub_allocation_index = i;
            temp_sub_allocation_count = 0;
        }

        // We stayed within a free allocation
        if (within_free && free) {
            temp_sub_allocation_count += 1;
        }
        
        // Check if the counted free slots suffice
        uint free_vertex_length = temp_sub_allocation_count * vertices_per_sub_allocation;
        uint free_triangles_length = temp_sub_allocation_count * triangles_per_sub_allocation;
        
        // If this is a possible candidate for a memory alloc offset, then use it
        if (within_free && free_vertex_length > counters.vertices && free_triangles_length > counters.triangles) {
            atomicMin(chosen_sub_allocation_index, temp_sub_allocation_index);
            invocation_local_chosen_sub_alloction_index = temp_sub_allocation_index;
            break;
        }

        // Update to take delta
        within_free = free;
    }

    barrier();
    memoryBarrier();

    // Only let one invocation do this shit
    if (invocation_local_chosen_sub_alloction_index != chosen_sub_allocation_index) {
        return;
    } 

    // Doesn't really matter since we can calculate it anyways 
    uint chosen_sub_allocation_count = int(max(ceil(float(counters.vertices) / float(vertices_per_sub_allocation)), ceil(float(counters.triangles) / float(triangles_per_sub_allocation))));

    // After finding the right block, we can write to it
    for (uint i = chosen_sub_allocation_index; i < (chosen_sub_allocation_index + chosen_sub_allocation_count); i++) {
        indices.data[i] = push_constants.chunk_index;
    }

    // Offsets that we will write eventually
    offsets.vertices = chosen_sub_allocation_index * vertices_per_sub_allocation;
    offsets.triangles = chosen_sub_allocation_index * triangles_per_sub_allocation;
}